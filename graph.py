from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START, END
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode, tools_condition

# for system prompt from langchain
from langchain.schema import SystemMessage

import os

# Tools
@tool
def run_command(cmd: str):
    """
    Takes a command line prompt and execute in the terminal and returns the output of the command
    eg: ls -> list all the files
    """
    result = os.system(command=cmd)
    return result
    
# create tool node for langgraph (from prebuild langgraph)
tool_node = ToolNode(tools=[run_command])

llm = init_chat_model(model_provider="openai", model="gpt-4.1")

# binding the tools
llm_with_tool = llm.bind_tools(tools=[run_command])

class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]
    
def chatbot(state: State):
    SYSTEM_PROMPT = SystemMessage(content="""
                                  You are an AI coding assistant who takes the input from the user and based on available tools you choose the correct tool and execute the command.
                                  
                                  You can even execute the commands and help user with the output of the command
                                  
                                  WHEN YOU GENERATE ANY FILE PLEASE ADD COMMENT ON TOP: "GENERATED BY AI"
                                  
                                  Always make sure to keep you generated codes and files into chat_gpt folder, if it is not present please create one in current dir

                                  After executing the command, please give the summary of the operation you did in 1-2 lines
                                  """)
    
    message = llm_with_tool.invoke([SYSTEM_PROMPT] + state["messages"])
    
    if not message.tool_calls:
        # If there are no tool calls, just return the message
        return {"messages": state["messages"] + [message]}
    
    # Return the tool calls to be processed by the tool node
    return {"messages": state["messages"] + [message], "tool_calls": message.tool_calls}

# initializing graph
graph_builder = StateGraph(State)

# creating nodes
graph_builder.add_node("chatbot", chatbot)
graph_builder.add_node("tools", tool_node)

# creating edges
graph_builder.add_edge(START, "chatbot")
# checking for tool condition
graph_builder.add_conditional_edges("chatbot", tools_condition)
# after tool condition will goto chatbot again
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge("chatbot", END)

# creates a new graph with given checkpointer
def create_chat_graph(checkpointer):
    return graph_builder.compile(checkpointer=checkpointer)