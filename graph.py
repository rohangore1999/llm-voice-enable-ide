from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START, END
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode, tools_condition

# for system prompt from langchain
from langchain.schema import SystemMessage

import os

# Tools
@tool
def run_command(cmd: str):
    """
    Takes a command line prompt and execute in the terminal and returns the output of the command
    eg: ls -> list all the files
    """
    result = os.system(command=cmd)
    result
    
# create tool node for langgraph (from prebuild langgraph)
tool_node = ToolNode(tools=[run_command])

llm = init_chat_model(model_provider="openai", model="gpt-4.1")

# binding the tools
llm_with_tool = llm.bind_tools(tools=[run_command])

class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]
    
def chatbot(state: State):
    SYSTEM_PROMPT = SystemMessage(content="""
                                  You are an AI coding assistant who takes the input from the user and based on available tools you choose the correct toll and execute the command.
                                  
                                  You can even execute the commands and help user with the output of the command
                                  
                                  WHEN YOU GENERATE ANY FILE PLEASE ADD COMMENT ON TOP: "GENERATED BY AI"
                                  
                                  Always make sure to keep you generated codes and files into chat_gpt folder, if it is not present please create one in current dir

                                  After executing the command, please give the summary of the operation you did in 1-2 lines
                                  """)
    
    message = llm_with_tool.invoke([SYSTEM_PROMPT] + state["messages"])
    # The message object is a dictionary that contains the LLM's response and any tool calls it made.

    """
    the assertion is checking that the number of tool calls in the message object is either 0 or 1 (not more than 1). This means the code is enforcing that the LLM can only make at most one tool call per message.
    """
    assert len(message.tool_calls) <= 1
    # only run if last messages is not a tool call
    # read existing messages and adding the llm messages
    return {"messages": [message]}

# initializing graph
graph_builder = StateGraph(State)

# creating nodes
graph_builder.add_node("chatbot", chatbot)
graph_builder.add_node("tools", tool_node)

# creating edges
graph_builder.add_edge(START, "chatbot")
# checking for tool condition
graph_builder.add_conditional_edges("chatbot", tools_condition)
# after tool condition will goto chatbot again
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge("chatbot", END)

# creates a new graph with given checkpointer
def create_chat_graph(checkpointer):
    return graph_builder.compile(checkpointer=checkpointer)